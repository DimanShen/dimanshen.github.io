---
published: true
title: Transformer 和 BERT
category: 自然语言处理
tags: 
  - 自然语言处理
  - Transformer
  - BERT
  - Self-Attention
layout: post
---

本文会先介绍计算机视觉和自然语言处理两个领域中训练任务方式的一些异同，然后对自然语言处理领域的 Transformer 和 BERT 模型做一些细节讲解和总结。

# CV vs. NLP

对于计算机视觉任务，由于图像本身就是数值矩阵，计算机可以直接处理而不存在表示方法的困扰，但对于自然语言处理，要让计算机能处理文本信息，首先需要解决文本的表示问题。

最初，有人用 One-hot 的表示方法将文本数值化，但这种表示过于高维、稀疏，并且无法表达词汇本身的语义信息。针对以上局限性，有人提出了 Word2vec 等无监督的方法，在语料库上训练得到每个词的词向量（Word Embedding）。词向量相较于 One-hot 向量要低维、稠密一些，此外，词向量本身是有含义的，其之间的相似度或者距离可以表示词汇间的语义相关性。目前，词向量是最广泛应用的文本表示方法。

对于不同的目标任务，NLP 最早的做法是根据每类任务定制不同的模型，输入预训练好的词向量，然后利用特定任务的数据集对模型进行训练。这样会存在一个问题：不是每个特定任务都有大量的标签数据可供训练，那些数据集非常小的任务将难以得到一个理想的模型。

针对以上问题，CV 领域已经有了一套成熟的解决方案：先用一个通用模型在大语料库上进行预训练，然后在特定任务上进行微调。具体来说，图像分类是计算机视觉中最基本的任务，当要解决一个小数据集的图像分类任务时，可先用一个通用网络模型在 ImageNet 上预训练得到一个规模庞大的模型，在此训练过程中，模型会不断学习如何提取特征，底层的 CNN 网络结构会提取边缘、纹理等通用特征，随着模型越往上走，提取的特征也越抽象。当完成预训练之后，可再根据具体图像任务调整最上层的网络结构，然后在该任务的小数据集上对模型进行微调，得到最终可用模型。以上方法较为完美地解决了特定任务数据不足的问题，而且对于各种任务不再需要从头开始训练网络，可以直接拿预训练好的结果进行微调，既减少了训练计算量的负担，也减少了人工标注数据的负担。

NLP 领域随后也引入了上述做法，接下来要介绍的 BERT 就是如此，其用一个已有的 Transformer 模型结构，提出了一整套的预训练和微调方法。首先，我们来看一下 Transformer 是什么。

# Transformer

[illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) 这篇文章由浅入深，很好地解释了 Transformer 的模型结构和训练过程，此处便不再赘述，以下只做一个简略总结。 

Transformer 由多层 Encoder 、多层 Decoder 和它们之间的连接层组成。输入第一层 Encoder 的 embedding 由句子中所有词的词向量与位置编码 (Positional Encoding) 相加得到。其中，位置编码可以通过训练过程中学习参数得到，也可以通过一个跟位置或时序相关的函数计算得到，比如正弦、余弦函数等。每个 Encoder 结构相同，均由一个 Multi-Head Self-Attention 结构和两层的 Feed Forward 全连接网络串联组成，且二者均有残差模块做 shortcut 和 normalization 做归一化的操作，这些有利于加速训练和提高模型的稳定性。Decoder 相比于 Encoder，在 Multi-Head Self-Attention 结构和全连接层之间还多了一个 Encoder-Decoder Attention 模块，以关注到输入句子的相关语义。这里需要注意的是，Encoder 或 Decoder 中每个子层的输入和输出向量维度都是相等的，比如，Multi-Head Self-Attention 的输入和输出向量维度是相等的，否则无法进行 shortcut 操作，全连接层的输入和输出向量维度是相等的，最终的输出和输入向量维度也是相等的，但每个自层内部的隐层向量维度会发生变化。

Multi-Head Self-Attention 结构是 Encoder/Decoder 算法单元中最重要的部分。下面我们分别解释什么是 Self-Attention，什么又是 Multi-Head。

Attention 会涉及到查询向量 $q$、键向量 $k$ 和值向量 $v$ 等概念，对于一般的 attention 来说，$k$ 和 $v$ 来源相同，而在此基础上若 $q$ 和 $k$、 $v$ 来源也相同，即为 Self-Attention。Self-Attention 的计算流程如下：sequence embedding 在进入到 attention 模块之前，有 3 个分叉，即从 1 个向量变成 3 个向量，具体来说，是定义三个矩阵：$W_Q$、$W_K$ 和 $W_V$，三者均随机初始化，后续通过训练优化。然后，embedding 和对应矩阵做乘法，分别得到 $q$、 $k$ 和 $v$。需要注意的是，$q$ 和 $k$ 要有相同的维度，$v$ 的维度可以相同，也可以不同，但一般也是相同的。接下来我们计算每一个 embedding 的输出，以句子中第一个词为例，用查询向量 $q_1$ 跟整句话每个键向量 $k_1$ ~ $k_n$ 分别做点积，得到 $n$ 个数值。然后除以一个常数得到 logits（尺度调整是为了易于训练），经过 softmax 函数得到 $n$ 个加和为 1 的系数权重 probability，将 $n$ 个系数与 $n$ 个值向量 $v_1$ ~ $v_n$ 进行加权求和，就得到了第一个词的输出向量 $z_1$。类似的，可以计算每个词的输出向量$z_1$ ~ $z_n$。通过这样一系列的计算，每个词的输出向量 $z$ 不再是孤立的，而都包含了其他词的信息，而且每个位置词与词的相关程度，可以通过 softmax 输出的权重进行分析。

Multi-Head 指对于同一组输入 embedding 可以并行做若干组上面的操作，例如，我们可以进行 8 组这样的运算，每一组都有各自的 $W_Q$、$W_K$ 和 $W_V$ 矩阵，不同组参数不共享，这样最终会计算出 8 组输出，将 8 组的输出拼接起来，并且乘以矩阵 $W_O$ 做一次线性变换，得到最终的输出。其中，$W_O$ 也随机初始化并在训练过程中被优化。Multi-Head 的好处在于：多个组可以并行计算，且不同的组可以捕获不同子空间的信息。

Encoder-Decoder Attention 的工作方式跟 Multi-Head Self-Attention 基本是一样的，除了一点：其 $q$ 来源于 Decoder 前层的输出，而 $k$ 和 $v$ 来源于 Encoder 最后一层。

最后，我们将 Transformer 与 RNN 进行一下对比。Transformer 的第一个优势在于：同样都是做 NLP 任务，RNN 结构对于每一个输出的隐向量 $h$， 包含的信息大多来源于当前的输入，距离越增加，信息衰减得越多，但是 Transformer 结构就不存在这个问题，不管当前词和其他词的空间距离有多远，包含其他词的信息量不取决于距离，而是取决于两者的相关性。此外，对于 Transformer 来说，在对当前词进行计算的时候，不仅可以用到前面的词，也可以用到后面的词，而 RNN 只能用到前面的词，不过，这点可以通过双向 RNN 来部分弥补。第三点是 RNN 的固有结构导致其计算效率不高：RNN 是一个顺序的结构，必须要一步一步地计算，只有计算出 $h_1$，才能计算 $h_2$，再计算 $h_3$，隐向量无法同时并行计算，但 Transformer 不存在这个问题。

# BERT

BERT，全称 Bidirectional Encoder Representations from Transformers，从名字我们就可以看到， BERT 只用到了 Transformer 中的 Encoder。堆叠Transformer、双向 Encoding、预训练语言模型，这几个词可被认为是 BERT 模型的核心。

BERT 的预训练包括了两个任务：Masked LM 和 Next Sentence Prediction。

第一个任务是随机地 mask 15% 的单词，用一个掩码 [MASK] 代替，然后训练模型去正确地预测出这个单词。这个任务可以让模型充分利用到上下文的信息，使其具有更强的表达能力，这也是 BERT 中 bidirectional 的含义。需要注意的是，在实际训练模型时，对于这 15% 的单词，并不是 100% 进行 mask，而会有部分保持不变或引入随机单词，这是为了保持 pre-training 与 fine-tuning 的一致，因为 fine-tuning 时并不能看到 [MASK] token，此外，随机单词引入的噪声可使模型具有一定纠错能力而更加稳健。

第二个任务是句子关系的判断。每个训练样本是一对上下句，有 50% 的样本下句和上句是真实的，另外 50% 的样本下句和上句是无关的，模型需要判断两句的关系。设置这个任务，是因为在一些 NLP 任务中需要判断句子关系，比如判断两句话是否有相同的含义，通过第二个任务，BERT 能够更好的捕捉句子之间的关系。

以上两个任务各有一个 loss，模型会将这两个 loss 加起来作为总的 loss 进行优化。

预训练完成后，即可针对特定任务对模型进行微调。以分类任务为例，可在预训练模型的输出向量中取出 [CLS] token 对应的向量 $c$，过一层新增的网络 $W$，再通过 softmax 进行分类，得到最终预测结果 $p$。这里在特定任务数据集上对预训练模型的所有参数和网络 $W$ 共同训练，直到收敛。新增加的网络 $W$ 是 $H$x$K$ 维，$H$ 表示隐向量的维度，$K$ 表示分类数量，$W$ 的参数数量远远小于预训练模型的参数，即微调的代价很小。

[超细节的BERT/Transformer知识点](https://zhuanlan.zhihu.com/p/132554155) 这篇文章提到了关于 BERT/Transformer 时间复杂度、权重共享、非线性、Embedding 相加、点积模型缩放等等很细致的知识点，有兴趣的同学可以进行拓展阅读。