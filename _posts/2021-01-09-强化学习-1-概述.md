---
published: true
title: 强化学习概述
category: 强化学习
tags: 
  - 强化学习
  - Policy-based
  - Value-based
layout: post
---

本文首先介绍强化学习的定义和相关基本概念，然后讲解 Policy-based 和 Value-based 的强化学习方法的区别与联系。

# 定义

> 强化学习是一种通过构建智能体（Agent）与环境（Environment）进行交互并从中学习，以解决决策问题（Decision Problems）的框架。其中，Agent 的学习指的是其在环境中反复试验（Trial and Error）并获得正向或负向的奖励（Reward）作为反馈的过程。

在强化学习中，我们构建一个 Agent，训练并优化它在环境中的决策能力。例如，能打败人类棋手的 AlphaGo、智能家居机器人等。为了能做出明智的决策，Agent 将通过反复与环境互动得到正向或者负向的 Reward 反馈，而从中获得经验或教训。

# 基本概念

强化学习的过程体现为状态、动作、奖励和下一个状态的循环，示意图如下：

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（一）/0.png" alt="0" style="zoom:80%;" />

强化学习两大主体是 **Agent** 和**环境**。Agent 在环境中生活并与之交互。每一次交互，Agent 都会获得对环境**状态**（State）整体或者局部的观察（Observation），然后决定它要采取的**行动**（Action）。环境会由于 Agent 的行为发生改变，也可能自主产生变化。

Agent 将从环境中获得**奖励**信号（通常是一个数值），由此判断当前环境状态的好坏。Agent 的目标是去最大化它能获得的累积奖励，强化学习也即 Agent 通过学习优化其行为以达到此目标的方法。

- ## 状态 & 观察

  状态和观察都是 Agent 能从环境中获取的信息。但二者存在如下区别：

  状态是对环境的完整描述，没有信息被隐藏。当 Agent 能够观察到环境的完整状态时，我们说该环境是被完全观察到的。例如在围棋任务中，Agent 能看到整个棋盘，那么它处于一个被完全观察到的环境，它接收到的是一个状态。

  而观察是对环境状态的局部描述，可能存在信息的缺失。当 Agent 只能获得部分观察时，我们说环境是被局部观察到的。例如在走迷宫时，Agent 只能看到眼前的路，并不能看到迷宫整体，那么它处于一个被局部观察到的环境，它接收到的则是一个观察。

  需要注意的是：在强化学习符号的标志约定中，有时会将状态符号 $s$ 放在某些 technically 更适合用观察符号 $o$ 的地方。具体来说是在 Agent 做出决策行为时，我们常说该行为是基于状态的，而在实践中，由于 Agent 无法获取状态，该行为其实是基于观察而做出的。

- ## 动作空间

  不同的环境中存在不同种类的动作，动作空间指在给定环境下所有有效动作的集合。

  动作空间存在两种类型：离散的与连续的。在离散的动作空间中，动作数量是有限的，例如在玩超级玛丽时，只有向四个方向移动以及跳跃等有限个动作；而在连续的状态空间中动作数量是无限的，例如现实生活中机器人的移动角度和速度可以是连续变化的。在连续动作空间中，动作一般用实数向量表示。

- ## 轨迹

  轨迹 $τ$ 是从初始时刻开始，由状态和动作交替组成的序列，它代表了 Agent 每一步决策的结果，也直接决定了将来可获得的奖励。
  
  $$
  τ = (s_0,\ a_0,\ s_1,\ a_1,\ ...)
  $$
  
  Agent 仅基于当前状态来决定下一步采取什么行动，而不考虑历史轨迹，所以强化学习过程是一种马尔科夫决策过程。

- ## 奖励

  奖励假设（Reward Assumption）是强化学习的中心思想，由于奖励假设的存在，Agent 对最佳行为的探索转化成了去最大化其预期累计奖励。
  
  $$
  R(τ) = r_0 + γr_1 + γ^2r_2 + γ^3r_3 + \ ...
  $$
  
  “预期”，表示对将来的估算，所以计算预期累计奖励，是指计算从当前状态出发到最终结束状态，沿着轨迹 $τ$ 可获得的奖励累加和。预期累计奖励也可以叫做**预期回报**。

  $γ$ 是取值 0~1 之间的一个折扣率，因为我们认为获得近在眼前的利益的可能性更大，而长远利益存在更多变数，所以我们用 $γ$ 对将来的奖励形成衰减。  

- ## Exploration & Exploitation

  Exploration 指采取随机的行为探索环境，以获取更多信息；Exploitation 指利用已知信息，获取当前可得的最大奖励。由于 Agent 一开始对环境知之甚少，所以初期进行 Exploration 更为重要，到后期则应偏重 Exploitation。强化学习中，在二者之间达到良好的平衡才有利于 Agent 做出最佳决策。

# Policy-based & Value-based

- ## Policy

  Policy 是 Agent 进行决策的依据，即指导 Agent 在给定状态下应该采取的行动。我们可以认为 Policy 是 Agent 的大脑，一般用 $\pi$ 表示。所以，要解决一个强化学习问题，我们要去优化 Policy $\pi$。最优策略（Optimal Policy）一般用 $\pi^*$ 表示，指的是通过训练找到的能获得最大预期回报的策略。

  有两种方法可以训练 Agent 找到最佳策略 $\pi^*$：基于策略的方法和基于值的方法。

- ## Policy-based

  Policy-based 的方法直接训练 Policy Function，教给 Agent 在给定状态下应该采取怎样的行动。所以，基于策略的方法直接去学习每个状态映射到的最佳动作，或者一组动作的概率分布。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（一）/1.png" alt="0" style="zoom:40%;" />

- ## Value-based

  Value-based 的方法会间接地训练一个 Value Function，而不是 Policy Function，Value Function 将状态映射到预期回报，即 Agent 可能获得的累计奖励。这种方法教导 Agent 去了解哪个状态更有价值，然后采取行动趋向更有价值的状态。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（一）/2.png" alt="0" style="zoom:40%;" />

需要注意的是：Value-based 的方法，虽然不同于 Policy-based 的方法去训练一种显示的、由状态到动作的映射，但并不是没有策略。我们可以认为其策略不那么具体、是指导性质的，比如始终让 Agent 去有最高价值的状态，是一种贪婪策略（Greedy Policy），而贪婪策略是基于值的方法中常用于指导 Agent 行为的策略。

总结来说，以上两种方法都是为了找到最佳策略 $\pi^*$，只是实现方式有所不同：基于策略的方法直接训练策略函数，不存在值函数；而基于值的方法通过找到最佳值函数来找到最佳策略。

