---
published: true
title: Q-Learning
category: 强化学习
tags: 
  - 强化学习
  - Value-based
  - Q-Learning
layout: post
---

本文会涉及 Value Function、Bellman 方程、$\epsilon$-greedy policy 等强化学习中的重要概念，并着重向大家介绍 Value-based RL 中的经典算法：Q-Learning。

# Value Function

我们在  [强化学习概述](https://dimanshen.github.io/强化学习/2021/01/09/强化学习-1-概述/) 一文中向大家介绍了 Value-based RL 的概念，其中 Value Function 是 Value-based 方法的核心，Value-based 方法通过寻找最佳 Value Function 获得最佳策略 Optimal Policy $\pi^*$。

Value Function 有两种类型：状态-值函数（State-Value Function）和动作-值函数（Action-Value Function），二者的区别是什么呢？

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/0.png" alt="0" style="zoom:60%;" />

State-Value Function 计算的是：从时间 $t$ 对应的状态 $s$ 出发，以后一直遵循指导 Agent 行为的 Policy $\pi$，所能获得的预期回报。这里$V_\pi(s)$ 代表状态 $s$ 的价值，$E_\pi(G_t)$ 代表时间 $t$ 对应的预期回报，$G$ 是收益 Gain 的缩写，含义等同于回报 Return。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/1.png" alt="0" style="zoom:60%;" />

Action-Value Function 计算的则是：从时间 $t$ 对应的状态 $s$ 出发，执行动作 $a$，且以后一直遵循指导 Agent 行为的 Policy $\pi$，所能获得的预期回报。这里 $Q(s,a)$ 代表 $(s,a)$ 这个状态-动作对所对应的价值，$Q$ 是 Quality 的缩写，代表了在 state $s$ take action $a$ 的质量好坏。

从迷宫示例图可以更直观地看到，State-Value Function 对于每个状态只做一次计算，对应的是 Policy $\pi$ 指导 Agent 在这个状态应该执行的动作所对应的价值，而 Action-Value Function 在每个状态则需要计算上下左右共四个动作的价值（图上没有全部用数字标识出来）。

总结来说：两者都是计算预期回报，其区别在于状态-值函数计算状态 $s$ 对应的预期回报，而动作-值函数计算的是状态-动作对 $(s,a)$ 的预期回报。

# Bellman Equation

如果我们要计算一个状态或状态-动作对的值，是否需要每次都对 Agent 从该状态开始到最后结束可获得的所有奖励进行求和呢？实际上是不需要的，这样会存在冗余的重复计算。

回到走迷宫的例子，每走 1 步，Agent 消耗 1 点体力，reward 为 -1，所以 Agent 希望走最短路径到达奖杯。那么在图中 $S_{t+1}$ 时，下一步应该往往哪个方向走呢？这取决于走一步之后到达的方格，哪个离奖杯更近。这里向上走是被挡住的，而向左或者向下走，都需要再折回原地，只有向右走是最佳选择。如果已知剩下的最短路径是走 5 步到达奖杯，那么 $S_{t+1}$ 对应的价值就应该是 -1 加 -5 等于 -6，同理 $S_t$ 对应的是 -1 加 -6 等于 -7。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/2.png" alt="0" style="zoom:60%;" />

抽象来说，其实这是一个动态规划的概念。我们认为在序列求解的过程中，如果一个解的轨迹是最佳轨迹，那么其中的每个片段都是当前的最佳轨迹。简单来讲：如果我们希望 $t$ 时刻在未来的影响达到最好，在 $t$ 时刻做出最佳选择之后，$t+1$ 时刻在未来的影响也应该要最好。

Bellman 方程就是一个对应最佳轨迹的递归方程，指的是我们可以将任何状态的预期回报视为：当前状态可获得的奖励 + 折扣率 * 下一个状态的预期回报。另外需要注意的是，虽然这里举例用的是 State-Value Function，但不论是 State-Value Function 还是 Action-Value Function，都可用 Bellman 方程来简化计算。

# Q-Learning

有了 Value Function 和 Bellman 方程的背景知识，现在进入 Q-Learning 的介绍。

### 定义

> Q-Learning 是一种 **off-policy**、**value-based** 的，用 **Temporal Difference（TD）**方式来训练其 **action-value function** 的强化学习方法。

加粗部分为 Q-Learning 定义的四个关键点，其中 Value-based 和 Action-Value Function 已介绍过，不再赘述。Off-policy 在后面结合算法流程会有更直观的讲解，至于 Temporal Difference，指的是每执行一次动作，都要对 Value Function 进行一次更新。与之相对应的是 Monte Carlo 方法，其在一整个 episode 结束后才计算一次预期回报，即利用完整的互动过程来更新 Value Function。

前面已经提到过，我们用 $Q(s,a)$ 代表 $(s,a)$ 状态-动作对的价值，这个值可叫做 $q$ 值。$Q$ 是 Quality 的缩写，代表了在 state $s$ take action $a$ 的质量好坏，所以这里对 Action-Value Function的优化就叫做 Q-Learning，而 Action-Value Function 也叫 Q-Function。

### Q-Table

这里引入了一个新的概念：Q-Table。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/3.png" alt="0" style="zoom:30%;" />

Q-Table 是存储所有 $q$ 值的一张表，就像是 Agent 在 Q-Learning 学习过程中的记忆，或者说是 Q-Function 的内部存储器。Q-Table 包含了所有状态-动作对的值，在给定状态和动作 $(s,a)$ 后，Q-Function 就可以在其中查找其相应的 $q$ 值。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/4.png" alt="0" style="zoom:60%;" />

既然 Q-Table 是 Agent 在学习过程中的记忆，那么最初，由于 Agent 对于环境还完全不了解，就像个新生的婴儿，所以 Q-Table 只能随机初始化。随着 Q-Learning 的进行，Agent 对环境进行探索获取经验，Q-Table 同时也会被更新，记录更准确的动作-状态对的价值。直至训练结束时，Q-Table 会收敛到最准确的状态，同时对应的 Q-Function 也就是最优的了。

### 算法流程

Q-Learning 算法步骤如下：

1. 设置超参，并初始化 Q-Table；
2. 基于 $\epsilon$-greedy policy 选择 action；
3. 执行 action，获得正向/负向 reward 并进入下一个 state；
4. 基于 greedy policy 更新 Q-Table；
5. 循环 2~4 步直至 Q-Table 收敛。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/5.png" alt="0" style="zoom:50%;" />

第一步是学习率和折扣率等超参的设置以及 Q-Table 的初始化。前面提到可以随机初始化 Q-Table，但在代码实现时，我们一般会将 Q-Table 全部值都设置为 0；

第二步是动作的选择，在这一步采用 $\epsilon$-greedy policy 来对 Exploration 和 Exploitation 二者进行平衡。$\epsilon$-greedy policy 指的是 Agent 每次 take action 时以 $\epsilon$ 的概率进行 exploration，1 - $\epsilon$ 的概率进行 exploitation。初始时，$\epsilon$ 的值将设置为一个很接近于1的实数，于是 Agent 以大概率进行 exploration，通过随机的 action 探索未知环境、获取信息，并记录在 Q-Table中。随着 Q-Table 的估算值越来越准确，$\epsilon$ 将逐渐减小，Agent 利用已知信息做出当前状态下的最佳选择，即进行 exploitation 的概率增大。

第四步是基于 greedy policy 更新 Q-Table。公式中的 TD Target 部分即 Bellman 方程的应用：任何状态的预期回报可被视为当前状态可获得的奖励 + 折扣率 * 的下一个状态的预期回报。蓝色的 $Q(S_t,A_t)$ 是还未更新的 Q-Table 的估算值，它和 TD Target 的差值再乘上一个学习率，就是要被更新的误差大小。到底什么叫做基于 greedy policy 来更新 Q-Table呢？回到 TD Target 的计算，我们看到计算下一个状态-动作对的预期回报用了一个 $max_a$ 符号，所以，greedy policy 指的是每次都使用在下一个状态能采取的最优动作所对应的 $q$ 值来进行 Q-Table的更新。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/6.png" alt="0" style="zoom:40%;" />

现在我们已经了解 Q-Learning的算法流程，让我们回过头来看看 Q-Learning 定义中的 off-policy 指的是什么。我们看到，第四步更新 Q-Table 时采用 greedy policy，而在第二步中 take action 采用的是 $\epsilon$-greedy policy。即在 Q-Learning 中，用来计算 TD Target 的 next action 不一定是下一步真实会 take 的 action。acting policy 和 updating policy 不一致，就叫做 off-policy。

和 Q-Learning 相对应的有个叫 Sarsa 的 Value-based 算法，它在第二步和第四步均采用 $\epsilon$-greedy policy，用来更新 Q-Table 的 next action 就是下一步真实会 take 的 action，这种 acting policy 和 updating policy 一致的情况叫做 on-policy。

### An example

为了更形象地展示 Q-Table 具体如何更新，我们来看一个简单的例子。假设迷宫中左上角有一只老鼠，想吃到右下角的大块芝士，同时要避免吃到毒药。一个 episode 有三种结束方式： 吃到毒药、吃到大块芝士或者已经走了五步。接下来我们按照 Q-Learning 的算法流程来训练老鼠学会最佳策略。

首先设置超参：学习率 = 0.1，折扣率 = 0.99，$\epsilon$ = 1，$reward_{空方格}$ = 0，$reward_{小芝士方格}$ = +1，$reward_{大芝士方格}$ = +10，$reward_{毒药方格}$ = -10。然后初始化 Q-Table，迷宫的6个方格对应6个状态，上下左右有4个动作，所以设置 Q-Table 为 6*4 的全零二维数组。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/7.png" alt="0" style="zoom:30%;" />

第一轮 acting and updating 如上图，假设老鼠进行 exploration，随机选择了往右走到达小芝士方格，于是获得 +1 的 reward 并更新 Q-Table 中 $Q(state_0,right)$ 对应值为 0.1。

<img src="https://raw.githubusercontent.com/DimanShen/dimanshen.github.io/master/_posts/image/强化学习（三）/8.png" alt="0" style="zoom:30%;" />

再循环一次第 2~4 步，如上图。假设 $\epsilon$ 值下降到 0.98，老鼠仍然进行 exploration，随机选择了往下走到达毒药方格，reward 为 -10并且老鼠被毒死了，当前 episode 结束。对应的，我们更新从小芝士方块、向下走这个状态-动作对的值 $Q(state_1,down)$ 。这里因为 episode 结束了，没有下一个状态，在计算时我们直接将 $max_aQ$ 这一项去掉。

我们看到经过两次 exploration，Q-Table 变得准确了点，之后随着更多 epsiode 循环第 2~4 步，Q-table 将对每个状态-动作对都有更好的估计。最后收敛时，我们就得到了一个准确的 Q-Table，也就有了一个最优的 Q-Function。
